{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes>=0.46.1"
      ],
      "metadata": {
        "id": "Tbq03g9v_mQP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vLohVKIy3Tq0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lora/qlora config\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "cadb260018de4e74be8008c5874e12ed",
            "891621c8c4e7427c8a308f8dc055c19d",
            "79c215fc3c5b4ab5a8c2464c5848d8cc",
            "04e3e7638f4f4ac0936af7fbbf1d33b5",
            "29ae89f22afb46bc929da0d331277746",
            "f4f9f23d86b14ce29357dfeb100a77fc",
            "d31ae6aef9354c4192a7a2a9b257e755",
            "88181b8257dd47f590eebca7151f1b39",
            "27ebdf3660e44386bbcde559f5e95e8d",
            "7ee5e40055744bd4a858c7c16d6f4918",
            "69b121cc6a3b4039a71ddf4159361ea3",
            "cd53740272644c87b46df827c0fd61a3",
            "06d61434f1a8435b9b47588a4c2fe3ff",
            "9895801aa4574de188c6cbc05ed75625",
            "163291454a2843b3941953d6276263cf",
            "0e7c40a7bce3409998ff3a3a9c0a3a90",
            "152dbaa12cfd4c4fb4f531106afd2868",
            "dbe7fcde4863450288f1aed510eb2ecd",
            "a6629963ea864233a8af2cd694f3721a",
            "dd878a64e42841aaa14cfba8dfc85501",
            "89ac935291144778b053f63eca519280",
            "b84e94d1305f46938d9e07b2f38dbeb4",
            "f5f58a39fad34a62bc67fe1d952e5d3b",
            "991ac612bcac43959b97144d62d10c3c",
            "8b70fefee6c8411c9a3adb548b1e149b",
            "3218ba0ff2714f53bc7436f8ee8c608e",
            "69fb0cf4029a4733b1d71bd511a8ca25",
            "9b4aeb9c80d14976a456c0eeb83e6243",
            "57e97b9f5e6b4f34a3bbb0ed5accf9bc",
            "5bdcbd3b975d457bbe92947b0d133112",
            "c7fd7e9e67c94ba4ad940b05b5d1ebd5",
            "12e2e4b3e0174a6c8370e77c88042159",
            "72d11a18c07a4ca6aeaa02c0bc2b3647",
            "c4cdb92150bd49c1a0c235ca5f3224d8",
            "dd86cd27f703408bb4779d65f2f346ba",
            "ed459c95914547a6a068b9d12724d998",
            "7b53607e8cec4f7680e6b0d74df11719",
            "08db062132f1465d8223e35f8ee5f440",
            "f9e38eaef0354775a04261ed7106efb1",
            "62eb890e12b84f0a9e5acca923e7021b",
            "e6392007c0e343ad93541229a06b28e3",
            "5b5f891a9bde46c7b0efba7207f1c80c",
            "536056653cb34822a2aba07a321917a9",
            "edd86d8bb101419bba2e832fb8b4a929",
            "da7f21b610d24433a40cc6e5a0623e0b",
            "0dd7b267eac143ebb7217d9b84698fa7",
            "7efeab8313524d10a2b85aebd99abd76",
            "a3b2e3d1ad254d00827e897ecca4a93a",
            "261441f10a494ea5b89d2d2b325de0bd",
            "a2fb3b75bc5f4b2d90eaa7655f7e9d0b",
            "ecf797b544f842d488a5497dcc7251da",
            "c959a10c91c649f99d5e52b0432254be",
            "e4f16ea00fcb4845ade62022cfe83373",
            "ef78e132e5fd4f039fae84d32708003a",
            "0a2dc2bcc8e54f13ac4ca1b5cb98f512",
            "fafb629fdd5544f7b29a729e9bd8ca7f",
            "332ab80a350b4c15bca36b844d0ea5bb",
            "f081de38f5134419b35e52a936c6d42f",
            "5e150f03e258462595c0a76a228ef480",
            "14fce9ec311b4c4b8379b368fd6efa84",
            "96c09939b2ef480eb01c59c2dc3c3485",
            "602ae9ee19ff42298a6fc1efbd05e618",
            "a9df488fc59149baa29df1f6a73a021b",
            "25a21e876dff4da5bb0d65d092f2ada4",
            "8335cd613fe049aa847e09ef20bc6c9d",
            "071d0d72966b46b3a4172a8325f3f8bf",
            "6b6fe400672a43419b64f1c864e5f36b",
            "288afec76ed04e61bc381d73e8a07c63",
            "29aae5f8f73f475e873d1ff7383374f8",
            "b0b27a1103f74b4b9d9bb2899029d35e",
            "37f7c7814f69411a825d3e5b30909008",
            "a3752b79955f4576bc8d4bbbc974b234",
            "3929cb75a6cf4ebfb7c71b3184a673db",
            "04511f6e68a5461a9f70b324abe5cff4",
            "dd6a3b69962f48ddbeb3697f23ab5f09",
            "6ed3ac003540435eb54c9eedc64a63d8",
            "6f5c3f3ed3214902ad514015e334202a"
          ]
        },
        "id": "przRjn-F9eZv",
        "outputId": "72e00bc4-1c03-46fd-f8b0-8cfab465f7d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cadb260018de4e74be8008c5874e12ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd53740272644c87b46df827c0fd61a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f58a39fad34a62bc67fe1d952e5d3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4cdb92150bd49c1a0c235ca5f3224d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da7f21b610d24433a40cc6e5a0623e0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fafb629fdd5544f7b29a729e9bd8ca7f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b6fe400672a43419b64f1c864e5f36b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r = 8,\n",
        "    lora_alpha = 16,\n",
        "    target_modules = ['q_proj','v_proj'],\n",
        "    lora_dropout = 0.05,\n",
        "    bias = 'none',\n",
        "    task_type = TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "oiU_kxNt_Wxg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "data = load_dataset(\"openai/gsm8k\",'main', split='train[:200]')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288,
          "referenced_widgets": [
            "c970bb72bc1a4bc7a74f85f2bfd073fc",
            "c99af2037e414e919fa87e9c1abdfaa9",
            "16bae804d3ff4b839beca7acdad6d347",
            "0d120265544440dba1a4ff14a377bcda",
            "63e0b6ad2af54210a685d69b1c974fc6",
            "865c210d19f7417fa7e5f888e3aa67e7",
            "e0b8b0040a664e499cf452e9099578d8",
            "c83d3ce8a32544e0aa1a2bc46db146a6",
            "c86c4616a59f4c85b5a7ffd2a9342d66",
            "86be0130214149ecaee079e98f00139e",
            "c5b7a20aed634a4c9d2a660320dc4936",
            "bb39cdaef0294db8b93a67fae6fd6626",
            "e41f1b07429a4353ae6a4f9949d2da7f",
            "f9ed99802dce4d058e5da23e4a60874c",
            "00ec994fbeeb46f69fe39d0c937a8756",
            "9de7ce07a01f47c587eac44ab9605311",
            "cdaacf12eeb542e2b0e4833189fa1fd8",
            "e7e1663a0762438c99b8a6e0add86f8f",
            "b8631dacce5e434384ee55245662599c",
            "5f853cafc5944884ad1855b2e94ce4f4",
            "bd0e69b172ec4e3097f663722fb7a7ed",
            "7f714491ebe846419d52e61dd3bbad13",
            "1dabfab2a08d47f7872ea793792da0a6",
            "566f7a54493a45e485eb4de05af9ead6",
            "b3cf1e6893cc4f7fa6f40bb8e5cce786",
            "ecf78f810f694fcebe11e15282f8870f",
            "739cebc713844c8997420ed527887984",
            "3cfebba261624c448ba88dfeaaa92ca8",
            "bdf9e05e4fe34ea9b6eacf5a7ab6b9e4",
            "c731cbbf9e3c4acf83b1d2b25ec29afd",
            "9e4af4bfcfc34dd1a364f2bd9e0b214d",
            "ee3be0cc3a424de786faf92ed3934c05",
            "1f5e2ec0f4fe4c74adbefc5a01435ae6",
            "1eabfc29f02b4e97bd6096096a8b3f39",
            "ece8f77a58fb48ccb16341a3222a2a6f",
            "4daf697912cf48f1bc7a32c7c099468f",
            "2131b99bf5df4e909bb28302b0d99e9b",
            "3af076f813d1453aa5d303fdef25f0b8",
            "4d2cd2ebad8c4e1da5d1903d0ba4cdf4",
            "161d89f6fbd8468dbadf0ccb1890babd",
            "307ed95c6fb44c6b88d44f5cf169fc3d",
            "ae4451a37b074c429ceaf60ddaa6572e",
            "436100c9a7df49ea8dddcd25e304df83",
            "83d18aa903654714b650afb55e14c2cd",
            "03b856d168454362bb695aa70e0a2645",
            "a9155d3f4cfb4c38bbbc8ab153063180",
            "c12995fc838b40a48f5b46610eb6f7e5",
            "a2638ff1e2b2433baabf0eb187b0b50b",
            "55e02494436146a1b3131976ba788c74",
            "fc2551e30c494478907fabcc12f7c772",
            "1789edd7fe1d435ca997f1070828a7a9",
            "91588e3d33fc48b79e3e84d3097b298a",
            "a7ba9fdc4f484b279c089d2e09bd0277",
            "7ee4d94599b8418ab086eff7b6a396d8",
            "df3b6526a5d04ce380e3b3890dc0bee9"
          ]
        },
        "id": "VS0yt_NrBIkr",
        "outputId": "cf311a59-82da-4249-eb59-6771a1818cb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c970bb72bc1a4bc7a74f85f2bfd073fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb39cdaef0294db8b93a67fae6fd6626"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1dabfab2a08d47f7872ea793792da0a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1eabfc29f02b4e97bd6096096a8b3f39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03b856d168454362bb695aa70e0a2645"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    texts = [\n",
        "        f\"### Instruction:\\n{instruction}\\n### Response:\\n{out}\"\n",
        "        for instruction, out in zip(batch[\"question\"], batch['answer'])\n",
        "    ]\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        texts,\n",
        "        padding = 'max_length',\n",
        "        max_length = 256,\n",
        "        truncation = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "\n",
        "    tokens['labels'] = tokens['input_ids'].clone()\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "FluNBhLUBeB0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = data.map(tokenize, batched=True, remove_columns=data.column_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "40f328b59abf42869e76a66eda71e28c",
            "ad808b6d93e74a88b0a33ec56ed59d3b",
            "fb8cd9f64a4040c8b22b0d881f26fa18",
            "49a4304f75c2491295c74c66e4dc47f5",
            "7f951b02ec434fbcba8f85b698a82ea2",
            "9f876b50457649a0864cabd3c23033c8",
            "3e114bd366254336b400c6a741071f33",
            "56ee1c5df33f4e92a53f98c2e5a21286",
            "c4ec44301ac0410ea4b1f1a1998dc1cd",
            "c1a826f83f444096acb52ea051693f80",
            "37e9066adf1d403ca5de207971df23fa"
          ]
        },
        "id": "vhUzHEI4CiCf",
        "outputId": "11b46358-91a3-40e6-d143-b3414f8771c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40f328b59abf42869e76a66eda71e28c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "37WFTkB0CrHl",
        "outputId": "b6abc4d4-f990-43cb-ea0c-b5b65317cc12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 200\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training config\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./tinyllama-lora\",\n",
        "    per_device_train_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    learning_rate = 2e-3,\n",
        "    num_train_epochs = 50,\n",
        "    fp16=True,\n",
        "    logging_steps = 20,\n",
        "    save_strategy = 'epoch',\n",
        "    report_to = 'none',\n",
        "    remove_unused_columns = False,\n",
        "    label_names = ['labels']\n",
        ")"
      ],
      "metadata": {
        "id": "amoVALyrCyM2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_data,\n",
        "    processing_class = tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "IzZaZRbVDmGk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-OX-IgdZD6jr",
        "outputId": "a3995291-aa56-4eac-ba3f-b60867d32cf7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [650/650 21:10, Epoch 50/50]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.506333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.745345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.615802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.495743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.379985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.300926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.232552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.169673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.136036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.101027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.082450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.062679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.052541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.045011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.041305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.035673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.034453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.031229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.029792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.029500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.027391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.026718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.025803</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.025643</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.025224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.025131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.024729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.024487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.024282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.024271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.023940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.023809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=650, training_loss=0.1674267552449153, metrics={'train_runtime': 1274.9103, 'train_samples_per_second': 7.844, 'train_steps_per_second': 0.51, 'total_flos': 1.590741172224e+16, 'train_loss': 0.1674267552449153, 'epoch': 50.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "model.save_pretrained(\"./tinyllama-fine-tuned-lora\")\n",
        "tokenizer.save_pretrained(\"./tinyllama-fine-tuned-lora\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTiCXTsoD-Kb",
        "outputId": "ff445c07-739e-46c3-b26a-89848e5e2509"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./tinyllama-fine-tuned-lora/tokenizer_config.json',\n",
              " './tinyllama-fine-tuned-lora/chat_template.jinja',\n",
              " './tinyllama-fine-tuned-lora/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import AutoModelForCausalLM,AutoTokenizer,BitsAndBytesConfig,default_data_collator\n",
        "\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "gBUyiV2WJjAF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "adapter_path = \"./tinyllama-fine-tuned-lora\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ").eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tuned_model = PeftModel.from_pretrained(\n",
        "    tmp_model,\n",
        "    adapter_path\n",
        ")\n",
        "tuned_model = tuned_model.merge_and_unload().eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "30765ba055bc4ab99fbd75eb15920fa7",
            "866c3d37f38346319f92f7aeed255963",
            "14957298838749988f046b860cdfd513",
            "7d719d7c08404c23b6aafc89e92a4e0f",
            "b15599d37228438189786281a33eeca8",
            "4745f469ee574fe59ab68c695a2c70e2",
            "83cefa19827549ca82835248faa0e75e",
            "fba3381692af4d3bb739063dd293fbb6",
            "150858dae1184074ad5a369ed8d5a175",
            "aa08156c3c144db1ad200e01fcdb41dc",
            "f9874c7ec389405698ef214e5b3ba9ab",
            "ec818ddfb19b4838a7a97644795c8feb",
            "7a122de64d4041a48dc2eeeb1150aff3",
            "6d55397c958649779f99e16a165738ac",
            "d1570813954c43799b2523d83de74b10",
            "21a0bcb4369044e9891ad0d8811a05db",
            "f3597dac9ec041b1a32c073f566f7a6a",
            "6cb4b89ea92a47b4a9f13a5b4939a073",
            "d13d283a1c80405b99c540d8a06e8b95",
            "0ce42a6af077473d81946b995448b98b",
            "0b96f7b57883436cbee7512798d8a202",
            "d2b812f2881d45c289a69edf77d71138"
          ]
        },
        "id": "iNJrVPReKcTx",
        "outputId": "325e4e17-8f7c-4591-c8ce-566474bdaccf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30765ba055bc4ab99fbd75eb15920fa7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec818ddfb19b4838a7a97644795c8feb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    texts = [\n",
        "        f\"### Instruction:\\n{instruction}\\n### Response:\\n{out}\"\n",
        "        for instruction, out in zip(batch[\"question\"], batch['answer'])\n",
        "    ]\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        texts,\n",
        "        padding = 'max_length',\n",
        "        max_length = 256,\n",
        "        truncation = True,\n",
        "        return_tensors = 'pt'\n",
        "    )\n",
        "\n",
        "    tokens['labels'] = tokens['input_ids'].clone()\n",
        "\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Ejrf6zu2LYxH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_ds = load_dataset(\"openai/gsm8k\",'main', split='train[200:300]')\n",
        "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=['question','answer'])\n",
        "eval_ds = eval_ds.with_format('torch')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "34595a9f9a5f4eb7973838e0780ea69f",
            "77ef5c993d544849ae461b777d7f8bd8",
            "d46c30911de744eb8f4ae81f202edd69",
            "b152f8d7ff7e4157a3ba99f83f3ae36c",
            "501db781c348424d9a3a355837ed8768",
            "dc32764bda364e819d2fd985d7af0afc",
            "daf3fd650ff74d4284b701df0c7f9196",
            "120de2fb6a2348049dcd2e634badb6ff",
            "d567f25fd889457d845fa79d7b194bd3",
            "9a833cc275e643319a38c4c61c6d55b2",
            "497c1ed9525f496a84e1870343c8fd7f"
          ]
        },
        "id": "m1y6ulIYLiiM",
        "outputId": "c3d52009-e0fe-4bfe-912a-6a26237bb1d7"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34595a9f9a5f4eb7973838e0780ea69f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loader = DataLoader(\n",
        "    eval_ds,\n",
        "    batch_size = 8,\n",
        "    collate_fn = default_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "p5yd3wRyL_90"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def compute_perplexity(model):\n",
        "    losses = []\n",
        "\n",
        "    for batch in eval_loader:\n",
        "        batch = {k:v.to('cuda') for k, v in batch.items()}\n",
        "        loss = model(**batch).loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return math.exp(sum(losses))/ len(losses)"
      ],
      "metadata": {
        "id": "oeb87ScbMNrC"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lower is better\n",
        "print(f\"Base Model Perplexity: {compute_perplexity(base_model):.2f}\")\n",
        "print(f\"Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxHWVoe9Mxna",
        "outputId": "dd6c808e-8195-472a-8502-be123fc07ce2"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Model Perplexity: 393165011551684504439471734784.00\n",
            "Tuned Model Perplexity: 19741409479.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "raw_data = load_dataset(\"openai/gsm8k\",'main', split='train[200:300]')\n",
        "refs = raw_data['answer']\n",
        "\n",
        "def generate(model, instruction):\n",
        "    token_ids = tokenizer(f'### Instruction:\\n{instruction}\\n### Response:\\n', return_tensors='pt').input_ids.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(token_ids, max_new_tokens=256)\n",
        "\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "6skGmrJzNZvH"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data['question'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "xDviLTXsQH2J",
        "outputId": "101785d7-388f-47ab-8bf9-6abed539d41a"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sansa is a famous artist, she can draw a portrait and sell it according to its size. She sells an 8-inch portrait for $5, and a 16-inch portrait for twice the price of the 8-inch portrait. If she sells three 8-inch portraits and five 16-inch portraits per day, how many does she earns every 3 days?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "refs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "bZkwKPCvQW3W",
        "outputId": "e757e239-4301-49ee-bb61-c1ba2f4583b6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sansa earns $5 x 3 = $<<5*3=15>>15 every day by selling three 8-inch portraits.\\nThe price of the 16-inch portrait is $5 x 2 = $<<5*2=10>>10 each.\\nSo, she earns $10 x 5 = $<<10*5=50>>50 every day by selling five 16-inch portraits.\\nHer total earnings is $50 + $15 = $<<50+15=65>>65 every day.\\nTherefore, the total amount she earns after 3 days is $65 x 3 = $<<65*3=195>>195.\\n#### 195'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(base_model, raw_data['question'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ5XeXRMQLgX",
        "outputId": "497e6778-da9d-43d0-c8f5-5b3cb6111293"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Sansa is a famous artist, she can draw a portrait and sell it according to its size. She sells an 8-inch portrait for $5, and a 16-inch portrait for twice the price of the 8-inch portrait. If she sells three 8-inch portraits and five 16-inch portraits per day, how many does she earns every 3 days?\n",
            "### Response:\n",
            "Sansa earns $100 per day, which means she earns $300 per week, and $1,200 per month. She sells 10 portraits per week, which means she earns $10 per week, and $50 per month. Therefore, she earns $100 per week, and $50 per month.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(tuned_model, raw_data['question'][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3llsTcMQdn8",
        "outputId": "02236c66-963d-40fc-b05a-cf25b6ba7cc0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Sansa is a famous artist, she can draw a portrait and sell it according to its size. She sells an 8-inch portrait for $5, and a 16-inch portrait for twice the price of the 8-inch portrait. If she sells three 8-inch portraits and five 16-inch portraits per day, how many does she earns every 3 days?\n",
            "### Response:\n",
            "Sansa earns each 8-inch portrait $5*1=<<5*1=5>>5.\n",
            "She earns each 16-inch portrait $2*2=<<2*2=4>>4.\n",
            "She sells 3 8-inch portraits and 5 16-inch portraits per day, which gives her a day's earnings of $5+$4=<<5+4=9>>9.\n",
            "She earns a total of $9*3=<<9*3=27>>27 every 3 days.\n",
            "#### 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IIt165ckQQHj"
      }
    }
  ]
}